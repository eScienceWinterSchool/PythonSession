{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.imgur.com/hkb7Bq7.png\" width=\"500\"></center>\n",
    "\n",
    "\n",
    "### **Prof. José Manuel Magallanes, PhD**\n",
    "\n",
    "* Professor, Departamento de Ciencias Sociales, Pontificia Universidad Católica del Perú, [jmagallanes@pucp.edu.pe](mailto:jmagallanes@pucp.edu.pe)\n",
    "\n",
    "* Visiting Professor, Evans School of Public Policy and Governance / Senior Data Science Fellow, eScience Institute, University of Washington, [magajm@uw.edu](mailto:magajm@uw.edu)\n",
    "_____\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='home'></a>\n",
    "\n",
    "# Introduction to Python\n",
    "\n",
    "### Using Python for Pre Processing\n",
    "\n",
    "In the session we will see the use of Python to:\n",
    "\n",
    "1. Collect data as dataframes into Python\n",
    "\n",
    "2. Preprocess a data frame:\n",
    "    * [Subset data](#subset)\n",
    "    * [Fix column names](#fixcolnames)\n",
    "    * [Look for non-standar missing values](#lookfornas)\n",
    "    * [Clean cell values](#cleancellvalues)\n",
    "    * [Format data types](#formatdtypes)\n",
    "\n",
    "\n",
    "3. Merge both tables:\n",
    "    * [Basic merge](#merging)\n",
    "    * [Fuzzy merge](#fuzzmerging)\n",
    "\n",
    "\n",
    "4. Prepare a file for further analysis\n",
    "    * [Scaling](#scaling)\n",
    "    * [Exporting](#exporting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect data tables into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of data file\n",
    "linkFile=\"https://hdr.undp.org/sites/default/files/2021-22_HDR/HDR21-22_Statistical_Annex_HDI_Table.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in a table from a file using pandas, since it is an Excel file, I requires **openpyxl**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available in my computer?\n",
    "!pip show openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not available, please go to Anaconda and install it. Once installed, or if available, continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the right function:\n",
    "import pandas as pd\n",
    "\n",
    "hdiFile=pd.read_excel(linkFile) # you might get an error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to add user-agent\n",
    "\n",
    "storage_options = {'User-Agent': 'Mozilla/5.0'}\n",
    "hdiFile = pd.read_excel(linkFile, storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Pre Processing\n",
    "\n",
    "<a id='subset'></a>\n",
    "\n",
    "### Subset data\n",
    "\n",
    "The object **hdiFile** is saving the information as a data frame. Just keep what you need; let's check the data head and tail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiFile.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiFile.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you do not see countries neither in the first rows nor in the last ones. There are rows at the begining and at the end that are not needed. \n",
    "\n",
    "Let's find the row indexes that have the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the problem is at the tail:\n",
    "hdiFile.tail(72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is this better?\n",
    "hdiFile.iloc[7:206,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting as new DF (copy)\n",
    "hdiRaw=hdiFile.iloc[7:206,:].copy()\n",
    "hdiRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "______\n",
    "\n",
    "<a id='fixcolnames'></a>\n",
    "\n",
    "### Fix column names\n",
    "\n",
    "Notice that **hdiRaw** do not have the right column names. So we need to recover them from **hdiFile**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiFile.iloc[[3,4],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the column names are in different positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and\n",
    "hdiFile.iloc[4,:2].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiFile.iloc[3,2:].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save column names \n",
    "RealHeaders=hdiFile.iloc[4,:2].to_list()+hdiFile.iloc[3,2:].to_list()\n",
    "# these are:\n",
    "RealHeaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's avoid all the \"ranks\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealHeaders[1:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep just those columns\n",
    "hdiRaw=hdiRaw.iloc[:,1:-3]\n",
    "hdiRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, put the right names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming\n",
    "hdiRaw.columns=RealHeaders[1:-3]\n",
    "#result:\n",
    "hdiRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have column names with missing values, let's get rid of those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BetterHeaders=hdiRaw.columns.dropna().to_list()\n",
    "#result\n",
    "BetterHeaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsetting again\n",
    "hdiRaw=hdiRaw.loc[:,BetterHeaders]\n",
    "hdiRaw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the columns:\n",
    "* Have acronyms in parenthesis.\n",
    "* Have spaces between words.\n",
    "\n",
    "Let's see what can be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bye anything between parenthesis\n",
    "hdiRaw.columns.str.replace('\\(.+\\)',\"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bye anything between parenthesis, bye leading-trailing spaces\n",
    "hdiRaw.columns.str.replace('\\(.+\\)',\"\", regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bye anything between parenthesis, bye leading-trailing spaces, title case\n",
    "hdiRaw.columns.str.replace('\\(.+\\)',\"\", regex=True).\\\n",
    "                          str.strip().\\\n",
    "                          str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep this last one for a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing column names\n",
    "hdiRaw.columns=hdiRaw.columns.str.replace('\\(.+\\)',\"\", regex=True).\\\n",
    "                          str.strip().\\\n",
    "                          str.title()\n",
    "#so\n",
    "hdiRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to decide how the we want as the shorter column name:\n",
    "\n",
    "* Same title without spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiRaw.columns.str.replace(\" \",'',regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some acronyms: Let's do this step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each column names splitted:\n",
    "[name.split() for name in hdiRaw.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first letter of each word\n",
    "[[word[0] for word in name.split()] for name in hdiRaw.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final result\n",
    "[''.join([word[0] for word in name.split()]) for name in hdiRaw.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the first alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiRaw.columns=hdiRaw.columns.str.replace(\" \",'',regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "______\n",
    "\n",
    "<a id='lookfornas'></a>\n",
    "\n",
    "### Look for non-standar missing values\n",
    "\n",
    "First check a cell that is full of non-word/non-digit characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full match!\n",
    "[hdiRaw.iloc[:,1].str.fullmatch(\"\\W+\",na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result is telling you if whether there is or there is not a full match (True/False). You can use that to keep the rows where this is _True_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick look...\n",
    "hdiRaw.iloc[:,1][hdiRaw.iloc[:,1].str.fullmatch(\"\\W+\",na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this for every column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "hdiRaw.iloc[:,i][hdiRaw.iloc[:,i].str.fullmatch(\"\\W+\",na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "hdiRaw.iloc[:,i][hdiRaw.iloc[:,i].str.fullmatch(\"\\W+\",na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **for** loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hdiRaw.shape[1]):\n",
    "    print(hdiRaw.iloc[:,i][hdiRaw.iloc[:,i].str.fullmatch(\"\\W+\",na=False)])\n",
    "# you might error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **try**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hdiRaw.shape[1]):\n",
    "    try:\n",
    "        print(hdiRaw.iloc[:,i][hdiRaw.iloc[:,i].str.fullmatch(\"\\W+\",na=False)])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the people who created this data set used \"..\" to represent **missing values**. Let's replace those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing !\n",
    "hdiRaw.replace(to_replace=[\"..\"],\n",
    "               value=None,\n",
    "               inplace=True)\n",
    "\n",
    "#result\n",
    "hdiRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "______\n",
    "\n",
    "<a id='cleancellvalues'></a>\n",
    "\n",
    "### Cleaning cell values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the current cell values have issues?\n",
    "\n",
    "* Keeping complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with all missing (after the first column)\n",
    "hdiRaw[hdiRaw.iloc[:,1:].isna().all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with at least one missing (after the first column)\n",
    "hdiRaw[hdiRaw.iloc[:,1:].isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, netx code will only keep complete data, and save it as a new data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiComplete=hdiRaw[~hdiRaw.iloc[:,1:].isna().any(axis=1)].copy()\n",
    "#\n",
    "hdiComplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Making sure columns of _text_ are clean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of leading and trailing spaces in text cells\n",
    "hdiComplete.Country=hdiComplete.Country.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Checking  numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiComplete.iloc[:,1:].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers have been recognised as **object** type. It might be due to having a non numeric value in one cell, or because it **had** a non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can you apply math?\n",
    "hdiRaw.iloc[:,1:].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just need to give format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiClean=hdiComplete.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "______\n",
    "\n",
    "<a id='formatdtypes'></a>\n",
    "\n",
    "### Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we just need to format the numeric columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Formatting into numeric type**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as easy as:\n",
    "hdiClean[hdiClean.columns[1:]]=hdiClean.iloc[:,1:].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recheck\n",
    "hdiClean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiFormat=hdiClean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "______\n",
    "\n",
    "\n",
    "## 3. Integrating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='merging'></a>\n",
    "\n",
    "### Basic merging\n",
    "\n",
    "As our data is clean and formatted (to the best of our knowledge), this process should be easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoFormat=pd.read_pickle(\"demoFormat.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoFormat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdiFormat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are confident we did a good cleaning and formatting, this step should be easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_on= / right_on NOT NEEDED (only when column names differ)\n",
    "hdiFormat.merge(demoFormat,left_on='Country', right_on='Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the amount of rowd **returned above**, and compare it with the amount of rows in each data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hdiFormat),len(demoFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest amount of rows is the maximum amount you expect during merge. Let's check the key values that were not matched:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyHDI=set(hdiFormat.Country)-set(demoFormat.Country)\n",
    "onlyDEMO=set(demoFormat.Country)-set(hdiFormat.Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyHDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyDEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "______\n",
    "\n",
    "\n",
    "<a id='fuzzmerging'></a>\n",
    "\n",
    "### Fuzzy Merge\n",
    "\n",
    "The previous objects (onlyDEMO, onlyHDI) inform the values not matched in the other data frame. \n",
    "If you want to recover some of these values, you may follow these steps (you may need to install **thefuzz**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import process as fz\n",
    "\n",
    "# take a country from onlyDEMO\n",
    "# and get the country that matches the most in OnlyHDI\n",
    "\n",
    "[(fz.extractOne(demo, onlyHDI),demo) for demo in sorted(onlyDEMO)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will not get the best outcome in this step, so you just need to keep the 'safe' matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(fz.extractOne(demo, onlyHDI),demo) for demo in sorted(onlyDEMO) \\\n",
    " if fz.extractOne(demo, onlyHDI)[1]>=90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is replace the cells values in one of the data frames.\n",
    "For that, you need to create a **dictionary of changes**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary is prepared for HDI data:\n",
    "{fz.extractOne(demo, onlyHDI)[0]:demo for demo in sorted(onlyDEMO) \\\n",
    " if fz.extractOne(demo, onlyHDI)[1]>=90}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW create the dict and make the changes\n",
    "changesHDI={fz.extractOne(demo, onlyHDI)[0]:demo \\\n",
    "            for demo in sorted(onlyDEMO) \\\n",
    "            if fz.extractOne(demo, onlyHDI)[1]>=90}\n",
    "\n",
    "# replace in HDI\n",
    "\n",
    "hdiFormat.Country.replace(to_replace=changesHDI,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# did you get more rows?\n",
    "hdiFormat.merge(demoFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you redo this process, you may recover more rows. I will not do it here, but you are welcome to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: start with these two lines!\n",
    "onlyHDI=set(hdiFormat.Country)-set(demoFormat.Country)\n",
    "onlyDEMO=set(demoFormat.Country)-set(hdiFormat.Country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our merge ended with one fuzzy-merge iteration; the data frame to use further will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdidem=hdiFormat.merge(demoFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format should still be good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdidem.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "______\n",
    "\n",
    "\n",
    "## Prepare file for further work\n",
    "\n",
    "<a id='scaling'></a>\n",
    "\n",
    "###  Scaling\n",
    "\n",
    "It would be good to check the range of values of your numeric data. You can simply use **describe** (just requesting _min_ and _max_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdidem.describe().loc[['min','max']].T #T for transposing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see different ranges, it would be good to request a **boxplot** (make sure to install **matplotlib** if not previously installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hdidem.plot(kind='box', rot=90,fontsize=5)\n",
    "plt.semilogy();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our concern is the numeric data. In case of categorical it is unusual to worry about it, but some cases might need some thinking.\n",
    "\n",
    "Let me get the column names of the numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "colsToScale = hdidem.select_dtypes([np.number]).columns\n",
    "\n",
    "colsToScale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to produce new ranges (make sure you have previously install **scikit-learn**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_minmax = scaler.fit_transform(hdidem.loc[:,colsToScale].to_numpy())\n",
    "df_scaled = pd.DataFrame(df_minmax, columns=colsToScale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_scaled.describe().loc[['min','max']].T \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.plot(kind='box', rot=90,fontsize=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a suffix to the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.columns=df_scaled.columns+\"_mM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat to the right (instead of bottom) with axis=1\n",
    "pd.concat([hdidem,df_scaled],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is our last version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdidem_plus=pd.concat([hdidem,df_scaled],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#home)\n",
    "______\n",
    "\n",
    "\n",
    "<a id='exporting'></a>\n",
    "\n",
    "### Exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For future use in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdidem_plus.to_pickle(\"hdidem_plus.pkl\")\n",
    "# you will need: DF=pd.read_pickle(\"hdidem_plus.pkl\")\n",
    "# or:\n",
    "# from urllib.request import urlopen\n",
    "# DF=pd.read_pickle(urlopen(\"https://...../hdidem_plus.pkl\"),compression=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For future  use in R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "base = importr('base')\n",
    "base.saveRDS(hdidem_plus,file=\"hdidem_plus.RDS\")\n",
    "\n",
    "#In R, you call it with: DF = readRDS(\"hdidem_plus.RDS\")\n",
    "#or, if read from cloud: DF = readRDS(url(\"https://...../hdidem_plus.RDS\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
